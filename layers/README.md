# Layers介绍

## 介绍
- [1] [attention/ self-attention/ seq2seq/ transformer, 含transformer实现](https://www.cnblogs.com/Lee-yl/p/11417506.html "attention/ self-attention/ seq2seq/ transformer")
- [2] [soft attention、hard attention、 local attention结构](https://blog.csdn.net/qq_16555103/article/details/99760588 "soft attention、hard attention、 local attention结构")
- [3] [自然语言处理中的Attention机制总结](https://blog.csdn.net/hahajinbu/article/details/81940355 "自然语言处理中的Attention机制总结")
- [4] [NLP中的各种MASK介绍](https://zhuanlan.zhihu.com/p/139595546)
- [5] [Transformer中的mask](https://blog.csdn.net/qq_35169059/article/details/101678207)
- [6] [bidirecitonal_dynamic_rnn中sequence_length的理解](https://blog.csdn.net/dunlongzun8445/article/details/89454610)
- [7] [[tf.layers.batch_normalization()介绍](https://www.cnblogs.com/fclbky/p/12636842.html)
- [8] [Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift](https://arxiv.org/abs/1502.03167)
- [9] [CNN中BN加在什么位置？](https://www.zhihu.com/question/45270958)

## 实现参考
- [1] [BILSTMAtt参考：Attention-Based Bidirectional Long Short-Term Memory Networks for Relation Classification](https://www.aclweb.org/anthology/P16-2034/ "Attention-Based Bidirectional Long Short-Term Memory Networks for Relation Classification")
- [2] [层次LSTMAtt参考：Hierarchical Attention Networks for Document Classification](https://www.aclweb.org/anthology/N16-1174/ "Hierarchical Attention Networks for Document Classification")
- [3] [多头注意力参考代码](https://github.com/TobiasLee/Text-Classification/blob/master/models/modules/multihead.py "多头注意力")
- [4] [Transformer Pytorch实现](https://www.cnblogs.com/zingp/p/11696111.html#_label9 "Transformer Pytorch实现")
- [5] [Text-Classification各种实现](https://github.com/TobiasLee/Text-Classification)

## License

[Apache License 2.0](LICENSE)

