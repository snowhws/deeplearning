# Layers介绍

## 介绍
- [1] [attention/ self-attention/ seq2seq/ transformer, 含transformer实现](https://www.cnblogs.com/Lee-yl/p/11417506.html "attention/ self-attention/ seq2seq/ transformer")
- [2] [soft attention、hard attention、 local attention结构](https://blog.csdn.net/qq_16555103/article/details/99760588 "soft attention、hard attention、 local attention结构")
- [3] [自然语言处理中的Attention机制总结](https://blog.csdn.net/hahajinbu/article/details/81940355 "自然语言处理中的Attention机制总结")
- [4] [NLP中的各种MASK介绍](https://zhuanlan.zhihu.com/p/139595546)
- [5] [Transformer中的mask](https://blog.csdn.net/qq_35169059/article/details/101678207)

## 实现参考
- [1] [BILSTMAtt参考：Attention-Based Bidirectional Long Short-Term Memory Networks for Relation Classification](https://www.aclweb.org/anthology/P16-2034/ "Attention-Based Bidirectional Long Short-Term Memory Networks for Relation Classification")
- [2] [层次LSTMAtt参考：Hierarchical Attention Networks for Document Classification](https://www.aclweb.org/anthology/N16-1174/ "Hierarchical Attention Networks for Document Classification")
- [3] [多头注意力参考代码](https://github.com/TobiasLee/Text-Classification/blob/master/models/modules/multihead.py "多头注意力")
- [4] [Transformer Pytorch实现](https://www.cnblogs.com/zingp/p/11696111.html#_label9 "Transformer Pytorch实现")

## License

[Apache License 2.0](LICENSE)

